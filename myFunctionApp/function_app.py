import azure.functions as func
import logging
import json
import re
import random


import os
import openai

from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import AnalyzeTextOptions
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError

# Cargar credenciales desde variables de entorno
OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT")
OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")

CONTENT_SAFETY_KEY = os.getenv("AZURE_CONTENT_SAFETY_API_KEY")
CONTENT_SAFETY_ENDPOINT = os.getenv("AZURE_CONTENT_SAFETY_API_URL")

app = func.FunctionApp()

def contains_inappropriate_language(prompt):
    inappropriate_words = [
        'mierda', 'idiota', 'estÃºpido', 'imbÃ©cil', 
        'pendejo', 'hijo de puta', 'gilipollas'
    ]
    
    prompt_lower = prompt.lower()
    
    for word in inappropriate_words:
        if word in prompt_lower:
            return True
    
    return False

def detect_prompt_intention(prompt):
    """
    AnÃ¡lisis mÃ¡s detallado de la intenciÃ³n del prompt
    """
    intention_patterns = {
        'learning': [
            r'\b(aprender|explicar|entender|comprender)\b',
            r'\b(cÃ³mo funciona|quÃ© es|significado)\b'
        ],
        'problem_solving': [
            r'\b(resolver|ayuda|soluciÃ³n|problema|diagnosticar)\b',
            r'\b(cÃ³mo puedo|quÃ© debo hacer)\b'
        ],
        'creative': [
            r'\b(imaginar|crear|diseÃ±ar|inventar|proponer)\b',
            r'\b(nueva idea|innovaciÃ³n|concepto)\b'
        ],
        'analytical': [
            r'\b(analizar|investigar|estudiar|examinar)\b',
            r'\b(impacto|consecuencias|desarrollo)\b'
        ],
        'technical': [
            r'\b(algoritmo|inteligencia artificial|machine learning|tecnologÃ­a)\b',
            r'\b(programaciÃ³n|cÃ³digo|sistema|computaciÃ³n)\b'
        ]
    }
    
    detected_intentions = []
    
    for intention, patterns in intention_patterns.items():
        for pattern in patterns:
            if re.search(pattern, prompt.lower()):
                detected_intentions.append(intention)
                break
    
    return detected_intentions if detected_intentions else ['general']

def detect_context_and_improve(prompt):
    """
    Detecta el contexto del prompt
    """
    context_patterns = {
        'learning': [
            r'\b(explicame|quÃ© es|cÃ³mo funciona)\b',
            r'\b(entender|comprender)\b'
        ],
        'problem_solving': [
            r'\b(resolver|problema|ayuda|soluciÃ³n)\b',
            r'\b(cÃ³mo puedo|quÃ© debo hacer)\b'
        ],
        'creative': [
            r'\b(imagina|crea|diseÃ±a|inventa|propÃ³n)\b',
            r'\b(nueva idea|innovaciÃ³n|concepto)\b'
        ],
        'technical': [
            r'\b(tecnologÃ­a|algoritmo|sistema|mÃ©todo)\b',
            r'\b(desarrollo|implementaciÃ³n)\b'
        ]
    }
    
    for context, patterns in context_patterns.items():
        for pattern in patterns:
            if re.search(pattern, prompt.lower()):
                return context
    
    return 'general'

def analyze_prompt_complexity(prompt):
    """
    Analiza la complejidad y profundidad del prompt
    """
    # MÃ©tricas de complejidad
    word_count = len(prompt.split())
    unique_words = len(set(prompt.lower().split()))
    
    # DetecciÃ³n de palabras tÃ©cnicas o especializadas
    technical_words = [
        'algoritmo', 'inteligencia', 'machine learning', 
        'neural', 'computacional', 'cuÃ¡ntico', 'blockchain'
    ]
    
    technical_word_count = sum(1 for word in technical_words if word in prompt.lower())
    
    # ClasificaciÃ³n de complejidad
    complexity_level = 'basic'
    if word_count > 10 and unique_words > 8:
        complexity_level = 'intermediate'
    if technical_word_count > 0 or word_count > 15:
        complexity_level = 'advanced'
    
    # AnÃ¡lisis de profundidad
    depth_indicators = {
        'basic': ['simple', 'general', 'introducciÃ³n'],
        'intermediate': ['detallado', 'explicaciÃ³n', 'anÃ¡lisis'],
        'advanced': ['profundo', 'complejo', 'especializado']
    }
    
    depth = random.choice(depth_indicators[complexity_level])
    
    return {
        'word_count': word_count,
        'unique_words': unique_words,
        'technical_words': technical_word_count,
        'complexity_level': complexity_level,
        'depth': depth
    }

def generate_improvement_suggestions(intentions, context, complexity=None):
    """
    Genera sugerencias de mejora basadas en la intenciÃ³n, contexto y complejidad
    """
    suggestion_templates = {
        'learning': [
            "Profundice en los conceptos fundamentales.",
            "Considere incluir ejemplos prÃ¡cticos para una mejor comprensiÃ³n."
        ],
        'problem_solving': [
            "Proporcione mÃ¡s detalles sobre el contexto del problema.",
            "Incluya informaciÃ³n sobre los recursos o herramientas disponibles."
        ],
        'creative': [
            "Explore lÃ­mites y restricciones para guiar la creatividad.",
            "Considere el impacto potencial de su idea innovadora."
        ],
        'technical': [
            "Especifique el nivel de conocimiento tÃ©cnico requerido.",
            "Considere el ecosistema tecnolÃ³gico relevante."
        ],
        'general': [
            "Sea mÃ¡s especÃ­fico en su solicitud.",
            "Proporcione contexto adicional para una mejor comprensiÃ³n."
        ]
    }
    
    # Combinar sugerencias basadas en intenciones y contexto
    suggestions = []
    for intention in intentions:
        if intention in suggestion_templates:
            suggestions.extend(
                random.sample(suggestion_templates[intention], 
                              min(1, len(suggestion_templates[intention])))
            )
    
    # Sugerencias basadas en complejidad
    if complexity:
        if complexity.get('complexity_level') == 'basic':
            suggestions.append("Considere expandir su prompt para obtener informaciÃ³n mÃ¡s detallada.")
        elif complexity.get('complexity_level') == 'advanced':
            suggestions.append("Su prompt parece ser muy tÃ©cnico. AsegÃºrese de que la audiencia comprenderÃ¡ la terminologÃ­a.")
    
    return suggestions
    
def analyze_content_safety(text, max_severity=1):
    """Analiza el texto y devuelve una respuesta en espaÃ±ol indicando posibles problemas y sugerencias."""
    result = {
        "original_prompt": text,
        "sanitized_prompt": text,
        "processed_prompt": None,
        "issues": [],
        "suggestions": [],
        "risk_level": "low",
        "intention": None,
        "context": None,
        "complexity": None
    }

    # Validar credenciales antes de ejecutar
    if not CONTENT_SAFETY_ENDPOINT or not CONTENT_SAFETY_KEY:
        logging.error("Las credenciales de Content Safety no estÃ¡n configuradas correctamente.")
        return {"error": "Error de configuraciÃ³n: faltan credenciales de Content Safety."}

    try:
        client = ContentSafetyClient(endpoint=CONTENT_SAFETY_ENDPOINT, credential=AzureKeyCredential(str(CONTENT_SAFETY_KEY)))
        response = client.analyze_text(AnalyzeTextOptions(text=text))

        # ðŸ” Imprimir la respuesta de Azure en logs
        logging.info(f"Respuesta de Azure: {response}")

        # Verificar si hay datos en categoriesAnalysis
        if not response.categories_analysis:
            logging.warning("La respuesta de Azure no contiene anÃ¡lisis de categorÃ­as.")
            return {"error": "No se recibieron datos de anÃ¡lisis de contenido."}

        # Procesar las categorÃ­as de la respuesta
        for category_analysis in response.categories_analysis:
            category = category_analysis.category.lower()  # Convertimos a minÃºsculas para evitar errores de comparaciÃ³n
            severity = category_analysis.severity

            if severity > max_severity:
                result["issues"].append(category)
                result["risk_level"] = "high"

        # Agregar sugerencias basadas en las categorÃ­as detectadas
        messages = {
            "hate": "Se detectÃ³ lenguaje de odio. Por favor, use un lenguaje respetuoso.",
            "selfharm": "Se detectaron indicios de autolesiÃ³n. Si necesita ayuda, por favor busque apoyo profesional.",
            "sexual": "El texto contiene contenido sexual. Considere reformularlo para mayor adecuaciÃ³n.",
            "violence": "Se detectÃ³ contenido violento. Evite promover la violencia en su mensaje."
        }

        for issue in result["issues"]:
            if issue in messages:
                result["suggestions"].append(messages[issue])
        logging.info(f"Respuesta de issue: {result}")
        return result
    
    except HttpResponseError as ex:
        logging.error(f"Error en el anÃ¡lisis de contenido: {ex.message}")
        return {"error": f"Error en la API de Azure: {ex.message}"}
    except Exception as e:
        logging.error(f"Error inesperado en el anÃ¡lisis de contenido: {str(e)}")
        return {"error": f"Error inesperado: {str(e)}"}

'''
def analyze_content_safety(text):
    try:
        logging.info(f"ENDPOINT: {CONTENT_SAFETY_ENDPOINT}")
        client = ContentSafetyClient(
            endpoint=CONTENT_SAFETY_ENDPOINT,
            credential=AzureKeyCredential(CONTENT_SAFETY_KEY),
            api_version="2024-09-01"
        )
        response = client.analyze_text(AnalyzeTextOptions(text=text))

        # ðŸ‘‡ DEBUG: imprime toda la respuesta del servicio
        logging.info(f"ðŸ” Resultado completo de Content Safety: {response}")
        
        # ðŸ‘‡ Forzamos logging detallado de cada categorÃ­a
        if hasattr(response, "categories_analysis"):
            for item in response.categories_analysis:
                logging.info(f"ðŸ“Œ CategorÃ­a: {item.category} - Severidad: {item.severity}")

        return [item.category for item in response.categories_analysis]  # si tu cÃ³digo original lo requiere

    except Exception as e:
        logging.error(f"âŒ Error en Content Safety: {str(e)}")
        return []
'''
def analyze_with_openai(prompt):
    """Usa Azure OpenAI para analizar el prompt y mejorar su calidad."""
    try:
        client = openai.AzureOpenAI(
            api_key=OPENAI_API_KEY,
            api_version=OPENAI_API_VERSION,
            azure_endpoint=OPENAI_ENDPOINT
        )
        
        response = client.chat.completions.create(
            model=OPENAI_DEPLOYMENT,
            messages=[{"role": "system", "content": "Corrige los errores ortogrÃ¡ficos y gramaticales en la pregunta del usuario sin cambiar su significado original. No elimines ni reemplaces palabras clave como 'AI', 'inteligencia artificial', 'machine learning', 'EE.UU.', 'CIA', 'datos', 'seguridad', etc. Si el usuario hace una pregunta confusa, corrige la gramÃ¡tica y la ortografÃ­a pero mantÃ©n el significado. Devuelve Ãºnicamente la pregunta corregida, sin agregar explicaciones ni comentarios adicionales."},
                      {"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=50
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"Error en OpenAI: {str(e)}")
        return None

@app.route(route="preprocess_prompt", methods=["OPTIONS", "POST"])
def preprocess_prompt_endpoint(req: func.HttpRequest) -> func.HttpResponse:
    # Manejando la solicitud OPTIONS (preflight)
    if req.method == "OPTIONS":
        return func.HttpResponse(
            "",
            status_code=200,
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            },
        )

    try:
        req_body = req.get_json()
        prompt = req_body.get('prompt', '')

        if not prompt:
            return func.HttpResponse(
                json.dumps({"error": "Prompt vacÃ­o"}),
                status_code=400,
                mimetype="application/json",
                headers={
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Methods": "POST, OPTIONS",
                    "Access-Control-Allow-Headers": "Content-Type",
                },
            )       
        
        # ðŸ”¹ 1. Inicializar resultado
        result = {
            'original_prompt': prompt,
            'sanitized_prompt': prompt,  # Solo cambiarÃ¡ si OpenAI se ejecuta para mejorar la pregunta si el content Safety detecta algo
            'processed_prompt': None,  #Solo cambiarÃ¡ si OpenAI se ejecuta el prompt y content safety pone riesgo no elevado.          
            'issues': [],
            'suggestions': [],
            'risk_level': 'low',
            'intention': None,
            'context': None,
            'complexity': None
        }

        # ðŸ”¹ 2. Analizar seguridad con Content Safety
        content_safety_result = analyze_content_safety(prompt)
        result['issues'].extend(content_safety_result.get('issues', []))
        result['suggestions'].extend(content_safety_result.get('suggestions', []))
        result['risk_level'] = content_safety_result.get('risk_level', 'low')
        
        # ðŸ”¹ 4. Si hay lenguaje inapropiado o riesgo alto, detener aquÃ­
        if 'inappropriate_language' in result['issues'] or result['risk_level'] == 'high':
            
            # âœ… Si el contenido es moderadamente riesgoso, lo reformulamos
            result['sanitized_prompt'] = analyze_with_openai(f"Reformula esta pregunta para que sea Ã©tica y segura: {prompt}")
            
            logging.info(f"Entro a lenguajes inapropiados o errores: {result}")
            if result['risk_level'] == 'high':
                return func.HttpResponse(
                    json.dumps(result),
                    status_code=200,
                    mimetype="application/json",
                    headers={
                        "Access-Control-Allow-Origin": "*",
                        "Access-Control-Allow-Methods": "POST, OPTIONS",
                        "Access-Control-Allow-Headers": "Content-Type",
                    },
                )
        
        # ðŸ”¹ 7. Ejecutar OpenAI solo si no hay lenguaje inapropiado
        improved_prompt = analyze_with_openai(result['sanitized_prompt'])
        if improved_prompt:
            result['processed_prompt'] = improved_prompt

        # ðŸ”¹ 8. Responder con los resultados
        return func.HttpResponse(
            json.dumps(result),
            status_code=200,
            mimetype="application/json",
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            },
        )
        

    except Exception as e:
        logging.error(f"Error en procesamiento: {str(e)}")
        return func.HttpResponse(
            json.dumps({"error": str(e)}),
            status_code=500,
            mimetype="application/json",
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            },
        )